<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>completion on P3troL1er的个人博客</title>
    <link>https://peterliuzhi.top/tags/completion/</link>
    <description>Recent content in completion on P3troL1er的个人博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>&amp;copy; 2023 &lt;a href=&#34;https://github.com/PeterLiu-all&#34;&gt;Peter Liu&lt;/a&gt;
</copyright>
    <lastBuildDate>Wed, 16 Nov 2022 14:13:51 +0800</lastBuildDate><atom:link href="https://peterliuzhi.top/tags/completion/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LinearRegression</title>
      <link>https://peterliuzhi.top/principle/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/simpleimplement/linearregression/</link>
      <pubDate>Wed, 16 Nov 2022 14:13:51 +0800</pubDate>
      
      <guid>https://peterliuzhi.top/principle/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/simpleimplement/linearregression/</guid>
      <description>The attempt to combine wisdom and power has only rarely been successful and then only for a short while. — Albert Einstein 线性回归的简单实现 定义函数 定义模型函数(X*W+b) 定义 Loss 函数1/2(y_hat - y)^2，计算网络所得结</description>
    </item>
    
    <item>
      <title>softmax简单实现</title>
      <link>https://peterliuzhi.top/principle/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/softmax/softmax%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Wed, 16 Nov 2022 14:13:51 +0800</pubDate>
      
      <guid>https://peterliuzhi.top/principle/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/softmax/softmax%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/</guid>
      <description>True friendship is like sound health; the value of it is seldom known until it is lost. — Charles Caleb Colton softmax 简单实现 需要导入的库 定义需要的函数 定义模型函数 定义交叉熵损失函数 定义准确率 定义优化算法 定义数据分批的函数 训</description>
    </item>
    
    <item>
      <title>线性回归优化实现-利用pytorch</title>
      <link>https://peterliuzhi.top/principle/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/simpleimplement/linearregression_optimized/</link>
      <pubDate>Wed, 16 Nov 2022 14:13:51 +0800</pubDate>
      
      <guid>https://peterliuzhi.top/principle/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/simpleimplement/linearregression_optimized/</guid>
      <description>You&amp;rsquo;re not obligated to win. You&amp;rsquo;re obligated to keep trying to do the best you can every day. — Marian Wright Edelman 我们之前的程序有时兼顾了太多的细节，pytorch的库里有许多优雅而方便的方法，我们可以利用它们来优化我们</description>
    </item>
    
  </channel>
</rss>
